---
globs: **/benches/**/*.rs,**/*bench*.rs,**/*performance*.rs
alwaysApply: false
---

# Performance Optimization Standards for DaemonEye

## High-Performance Concurrency Foundation

DaemonEye uses idiomatic best practices for high-performance concurrent systems:

- **Event Bus**: `crossbeam` crate for lock-free channels and efficient backoff strategies in collector-core
- **Async Runtime**: `tokio` primitives (`Semaphore`, `mpsc`, `oneshot`, `watch`, `Notify`) for bounded concurrency
- **Locking**: `parking_lot::RwLock` for better performance than standard `RwLock` in high-contention scenarios
- **Threading**: `std::thread::spawn` with `crossbeam::utils::Backoff` for efficient spinning in event routing

## Performance Targets

DaemonEye must meet strict performance requirements:

- **CPU Usage**: < 5% sustained during continuous monitoring
- **Memory Usage**: < 100 MB resident under normal operation
- **Process Enumeration**: < 5s for 10,000+ processes
- **Database Operations**: > 1,000 records/second write rate
- **Alert Latency**: < 100ms per detection rule execution
- **Query Response**: Sub-second response times for 100,000+ events/minute (Enterprise)

## Benchmarking with Criterion

Use Criterion for performance benchmarking:

```rust
use criterion::{black_box, criterion_group, criterion_main, Criterion};

fn benchmark_process_collection(c: &mut Criterion) {
    let mut group = c.benchmark_group("process_collection");

    group.bench_function("collect_processes", |b| {
        b.iter(|| {
            let collector = ProcessCollector::new();
            black_box(collector.collect_processes())
        })
    });

    group.bench_function("collect_processes_parallel", |b| {
        b.iter(|| {
            let collector = ProcessCollector::new();
            black_box(collector.collect_processes_parallel())
        })
    });

    group.finish();
}

fn benchmark_database_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("database_operations");

    group.bench_function("insert_process", |b| {
        let db = Database::new(":memory:").unwrap();
        let process = ProcessInfo::default();

        b.iter(|| black_box(db.insert_process(&process)))
    });

    group.finish();
}

criterion_group!(
    benches,
    benchmark_process_collection,
    benchmark_database_operations
);
criterion_main!(benches);
```

## Memory Management

Implement bounded memory usage patterns with idiomatic concurrency:

```rust
use tokio::sync::Semaphore;
use parking_lot::RwLock;
use std::sync::Arc;

// Bounded concurrency pattern with tokio Semaphore
let semaphore = Arc::new(Semaphore::new(64));
let permit = semaphore.acquire().await?;

// Use parking_lot::RwLock for better performance in high-contention scenarios
let shared_state = Arc::new(RwLock::new(Vec::new()));

// Critical: no .await calls while holding locks
{
    let _guard = shared_state.write();
    // Synchronous work only inside lock scope
    process_data_synchronously(&data);
    // Lock dropped here
}
// Async work after lock is released
perform_async_operation().await?;
drop(permit);
```

## Database Optimization

Optimize redb database operations:

```rust
// Use batch operations for high throughput
let mut batch = db.batch_write();
for process in processes {
    batch.insert_process(&process)?;
}
batch.commit()?;

// Use redb TableDefinition and transactions for efficient queries
use redb::{Database, ReadableTable, TableDefinition};

const PROCESSES_TABLE: TableDefinition<u32, ProcessInfo> = TableDefinition::new("processes");

// Read transaction for queries
let read_txn = db.begin_read()?;
let table = read_txn.open_table(PROCESSES_TABLE)?;
let result = table.get(pid)?.map(|guard| guard.value().clone());

// Write transaction for inserts/updates
let write_txn = db.begin_write()?;
{
    let mut table = write_txn.open_table(PROCESSES_TABLE)?;
    table.insert(pid, &process_info)?;
}
write_txn.commit()?;

// Note: If SQL/prepared-statement semantics are required, use an SQL database
// (SQLite/Postgres) or a Rust SQL wrapper like sqlx or diesel
```

## Async Performance

Use Tokio runtime efficiently with crossbeam for high-performance event distribution:

```rust
// Prefer tokio::sync primitives for async code
use tokio::sync::{Semaphore, mpsc, oneshot, watch, Notify};
use crossbeam::channel::{bounded, unbounded};
use parking_lot::RwLock;

// Bounded channels for backpressure (tokio for async, crossbeam for sync)
let (tx, rx) = mpsc::channel::<Event>(1000);  // Async context
let (sync_tx, sync_rx) = bounded::<Event>(1000);  // Sync context

// Use Semaphore for bounded concurrency
let semaphore = Arc::new(Semaphore::new(10));

// High-performance event bus with crossbeam channels
let (event_tx, event_rx) = unbounded::<BusEvent>();
let subscribers = Arc::new(RwLock::new(HashMap::new()));

// Efficient backoff for event routing
use crossbeam::utils::Backoff;
let backoff = Backoff::new();
```

## Resource Management

Implement proper resource cleanup with high-performance locking:

```rust
use std::sync::Arc;
use parking_lot::RwLock;
use tokio::sync::Semaphore;

struct ResourceManager {
    resources: Arc<RwLock<Vec<Resource>>>,
    semaphore: Arc<Semaphore>,
}

impl ResourceManager {
    async fn cleanup_resources(&self) {
        let _permit = self.semaphore.acquire().await;
        {
            let mut resources = self.resources.write();
            for resource in resources.drain(..) {
                resource.cleanup();
            }
        }
        // Permit dropped here, allowing other operations
    }
}

impl Drop for ResourceManager {
    fn drop(&mut self) {
        // Ensure resources are properly cleaned up
        if let Ok(mut resources) = self.resources.try_write() {
            for resource in resources.drain(..) {
                resource.cleanup();
            }
        }
    }
}
```

## Event Bus Performance Patterns

Implement high-performance event distribution with crossbeam:

```rust
use crossbeam::channel::{bounded, unbounded, Receiver, Sender};
use crossbeam::utils::Backoff;
use parking_lot::RwLock;
use std::thread;
use std::sync::Arc;
use std::collections::HashMap;

// High-performance event bus implementation
pub struct HighPerformanceEventBus {
    publisher: Sender<BusEvent>,
    subscribers: Arc<RwLock<HashMap<String, SubscriberInfo>>>,
    routing_handle: Option<thread::JoinHandle<()>>,
}

impl HighPerformanceEventBus {
    pub fn new() -> Self {
        let (publisher, receiver) = unbounded::<BusEvent>();
        let subscribers = Arc::new(RwLock::new(HashMap::new()));

        // Spawn routing thread with crossbeam backoff
        let routing_handle = thread::spawn(move || {
            let backoff = Backoff::new();
            while let Ok(event) = receiver.recv() {
                // Route to subscribers with efficient backoff
                let subs = subscribers.read();
                for (_, subscriber) in subs.iter() {
                    if let Err(_) = subscriber.sender.try_send(event.clone()) {
                        backoff.snooze(); // Efficient spinning
                    }
                }
            }
        });

        Self {
            publisher,
            subscribers,
            routing_handle: Some(routing_handle),
        }
    }
}
```

## Performance Testing

Include performance regression tests:

```rust
#[tokio::test]
async fn test_performance_requirements() {
    let start = Instant::now();

    // Test process collection performance
    let collector = ProcessCollector::new();
    let processes = collector.collect_processes().await.unwrap();

    let duration = start.elapsed();

    // Must complete within 5 seconds for 10k+ processes
    assert!(duration < Duration::from_secs(5));
    assert!(processes.len() > 1000);
}
```

## Memory Leak Detection

Test for memory leaks:

```rust
#[tokio::test]
async fn test_memory_usage() {
    let initial_memory = get_memory_usage();

    // Run operations that should not leak memory
    for _ in 0..1000 {
        let collector = ProcessCollector::new();
        let _processes = collector.collect_processes().await.unwrap();
        drop(collector);
    }

    // Force garbage collection
    tokio::task::yield_now().await;

    let final_memory = get_memory_usage();
    let memory_increase = final_memory - initial_memory;

    // Memory increase should be minimal
    assert!(memory_increase < 10 * 1024 * 1024); // 10MB
}
```
